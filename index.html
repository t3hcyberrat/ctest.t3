<!DOCTYPE html>
<html>

<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">

<title>Experiment_clothing_0</title>

<!-- import aframe and then ar.js with image tracking / location based features -->
<script
    src="https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js"></script>
<script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js"></script>

<script>

    AFRAME.registerComponent('markerhandler', {
        init: function () {
                this.toggle = false;
                document.querySelector("#vid").pause(); //reference to the video
        },
        tick: function () {
            this.sceneEl.addEventListener('markerFound', (event) => {
                console.log(event)
            });
            // if (this.sceneEl.addEventListener('markerFound') == true) {
            //     console.log("found");
            // } else {
            //     console.log("lost");
            // }
        }
    });

    // AFRAME.registerComponent('vidhandler', {
    //     init: function () {
    //     },
    //     tick: function () {
    //         if (document.querySelector("nft-marker").object3D.visible == true) {
    //             if (!this.toggle) {
    //                 this.toggle = true;
    //                 document.querySelector("#vid").play();
    //             }
    //         } else {
    //             this.toggle = false;
    //             document.querySelector("#vid").pause();
    //         }
    //     }
    // });
</script>


<!-- style for the loader -->
<style>
    .arjs-loader {
        height: 100%;
        width: 100%;
        position: absolute;
        top: 0;
        left: 0;
        background-color: rgba(0, 0, 0, 0.8);
        z-index: 9999;
        display: flex;
        justify-content: center;
        align-items: center;
    }

    .arjs-loader div {
        text-align: center;
        font-size: 1.25em;
        color: white;
    }
</style>

<body style="margin : 0px; overflow: hidden;">
    <!-- minimal loader shown until image descriptors are loaded. Loading may take a while according to the device computational power -->
    <div class="arjs-loader">
        <div>Loading, please wait...</div>
    </div>

    <!-- a-frame scene -->
    <a-scene vr-mode-ui="enabled: false;" renderer="logarithmicDepthBuffer: true;" embedded
        arjs="trackingMethod: best; sourceType: webcam; debugUIEnabled: true;">
        <!-- a-nft is the anchor that defines an Image Tracking entity -->
        <!-- on 'url' use the path to the Image Descriptors created before. -->
        <!-- the path should end with the name without the extension e.g. if file is 'pinball.fset' the path should end with 'pinball' -->
        <a-nft markerhandler type="nft" url="https://t3hcyberrat.github.io/ctest.t3/assets/test_4" smooth="true"
            smoothCount="10" smoothTolerance=".01" smoothThreshold="5">
            <!-- as a child of the a-nft entity, you can define the content to show. here's a GLTF model entity -->
            <!-- <a-entity gltf-model="assets/34.glb" scale="40 40 40" position="10 10 -100">
                </a-entity> -->

            <a-video id="#vid" width="160" height="100" rotation="-90" position="10 10 -100"
                src="assets/test_11.mp4"></a-video>
        </a-nft>
        <!-- static camera that moves according to the device movemenents -->
        <a-entity camera></a-entity>
    </a-scene>
</body>

</html>