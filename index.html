<!DOCTYPE html>
<html>

<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">

<title>Experiment_clothing_0</title>

<!-- import aframe and then ar.js with image tracking / location based features -->
<script
    src="https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js"></script>
<script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js"></script>

<script>

    AFRAME.registerComponent('markerhandler', {
        init: function () {

            this.el.sceneEl.addEventListener('markerLost', (event) => {
                document.getElementById("video").pause(); //reference to the video

                console.log("lost");
            });

            this.el.sceneEl.addEventListener('markerFound', (event) => {
                document.getElementById("video").play(); //reference to the video

                console.log("found");
            });
        },

    });
</script>


<!-- style for the loader -->
<style>
    .arjs-loader {
        height: 100%;
        width: 100%;
        position: absolute;
        top: 0;
        left: 0;
        background-color: rgba(0, 0, 0, 0.8);
        z-index: 9999;
        display: flex;
        justify-content: center;
        align-items: center;
    }

    .arjs-loader div {
        text-align: center;
        font-size: 1.25em;
        color: white;
    }
</style>

<body style="margin : 0px; overflow: hidden;">
    <!-- minimal loader shown until image descriptors are loaded. Loading may take a while according to the device computational power -->
    <div class="arjs-loader">
        <div>Loading, please wait...</div>
    </div>

    <!-- a-frame scene -->
    <a-scene vr-mode-ui="enabled: false;" renderer="logarithmicDepthBuffer: true;" embedded
        arjs="trackingMethod: best; sourceType: webcam; debugUIEnabled: true;">

        <a-assets>
            <!-- <video id="video" loop="true" src="assets/test_11.mp4"></video> -->
            <video id="video" loop="true" src="assets/T3st.mp4"></video>
        </a-assets>

        <!-- a-nft is the anchor that defines an Image Tracking entity -->
        <!-- on 'url' use the path to the Image Descriptors created before. -->
        <!-- the path should end with the name without the extension e.g. if file is 'pinball.fset' the path should end with 'pinball' -->
        <a-nft type="nft" url="https://t3hcyberrat.github.io/ctest.t3/assets/test_4" smooth="true" smoothCount="10"
            smoothTolerance=".01" smoothThreshold="5">
            <!-- as a child of the a-nft entity, you can define the content to show. here's a GLTF model entity -->
            <!-- <a-entity gltf-model="assets/34.glb" scale="40 40 40" position="10 10 -100">
                </a-entity> -->

            <a-video src="#video" markerhandler width="160" height="100" rotation="-90" position="10 10 -100"></a-video>
        </a-nft>
        <!-- static camera that moves according to the device movemenents -->
        <a-entity camera></a-entity>
    </a-scene>
</body>

</html>